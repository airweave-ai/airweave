"""Spotlight search planner.

The planner builds search plans by analyzing the user query, collection metadata,
and search history. It uses an LLM with structured output to generate plans.
"""

from pathlib import Path

from airweave.core.logging import ContextualLogger
from airweave.search.spotlight.external.llm.interface import SpotlightLLMInterface
from airweave.search.spotlight.external.tokenizer.interface import SpotlightTokenizerInterface
from airweave.search.spotlight.schemas.history import SpotlightHistory
from airweave.search.spotlight.schemas.plan import SpotlightPlan
from airweave.search.spotlight.schemas.state import SpotlightState


class SpotlightPlanner:
    """Plans search queries based on state and history.

    The planner:
    1. Builds a prompt from static context, collection metadata, user query, and history
    2. Manages token budget to fit history within context window
    3. Calls the LLM to generate a structured search plan
    """

    # Path to context markdown files (relative to this module)
    CONTEXT_DIR = Path(__file__).parent.parent / "context"

    # Reserve this fraction of context_window for reasoning (CoT)
    REASONING_BUFFER_FRACTION = 0.15

    def __init__(
        self,
        llm: SpotlightLLMInterface,
        tokenizer: SpotlightTokenizerInterface,
        logger: ContextualLogger,
    ) -> None:
        """Initialize the planner.

        Args:
            llm: LLM interface for generating plans.
            tokenizer: Tokenizer for counting tokens.
            logger: Logger for debugging.
        """
        self._llm = llm
        self._tokenizer = tokenizer
        self._logger = logger

        # Load static context files once
        self._airweave_overview = self._load_context_file("airweave_overview.md")
        self._planner_task = self._load_context_file("planner_task.md")

    def _load_context_file(self, filename: str) -> str:
        """Load a context markdown file.

        Args:
            filename: Name of the file in the context directory.

        Returns:
            Contents of the file.

        Raises:
            FileNotFoundError: If the file doesn't exist.
        """
        filepath = self.CONTEXT_DIR / filename
        return filepath.read_text()

    async def plan(self, state: SpotlightState) -> SpotlightPlan:
        """Generate a search plan based on the current state.

        Args:
            state: The current spotlight state.

        Returns:
            A search plan generated by the LLM.
        """
        prompt = self._build_prompt(state)

        # Log only the dynamic parts (not static airweave_overview and planner_task)
        history_section = SpotlightHistory.build_history_section(
            state.history, self._tokenizer, self._calculate_history_budget(0)
        )
        self._log_dynamic_context(state, history_section)

        prompt_tokens = self._tokenizer.count_tokens(prompt)
        self._logger.debug(f"[Planner] Total prompt: {prompt_tokens:,} tokens")

        plan = await self._llm.structured_output(prompt, SpotlightPlan)

        # Log the generated plan
        self._logger.info(f"[Planner] Generated plan:\n{plan.to_md()}")

        return plan

    def _log_dynamic_context(self, state: SpotlightState, history_section: str) -> None:
        """Log the dynamic parts of the prompt context.

        Only logs user query, collection metadata, iteration, and history.
        Static parts (airweave_overview, planner_task) are not logged.
        """
        self._logger.debug(
            f"[Planner] Dynamic context for iteration {state.iteration_number}:\n"
            f"{'=' * 60}\n"
            f"USER QUERY: {state.user_query}\n"
            f"{'=' * 60}\n"
            f"COLLECTION METADATA:\n{state.collection_metadata.to_md()}\n"
            f"{'=' * 60}\n"
            f"HISTORY:\n{history_section}\n"
            f"{'=' * 60}"
        )

    def _build_prompt(self, state: SpotlightState) -> str:
        """Build the full prompt for the LLM.

        Token budget is managed to fit history within the context window.
        History is added from most recent to oldest until budget is exhausted.

        Args:
            state: The current spotlight state.

        Returns:
            The complete prompt string.
        """
        # Build the static part of the prompt (without history)
        static_prompt = self._build_static_prompt(state)
        static_tokens = self._tokenizer.count_tokens(static_prompt)

        # Calculate token budget for history
        budget_for_history = self._calculate_history_budget(static_tokens)

        # Build history section with budget
        history_section = SpotlightHistory.build_history_section(
            state.history, self._tokenizer, budget_for_history
        )

        # Combine into final prompt
        return f"""{static_prompt}

---

## Search History

{history_section}
"""

    def _build_static_prompt(self, state: SpotlightState) -> str:
        """Build the static part of the prompt (everything except history).

        Args:
            state: The current spotlight state.

        Returns:
            The static prompt string.
        """
        return f"""# Airweave Overview

{self._airweave_overview}

---

{self._planner_task}

---

## Context for This Search

### User Query

{state.user_query}

### Collection Information

{state.collection_metadata.to_md()}

### Current Iteration

This is iteration **{state.iteration_number}**."""

    def _calculate_history_budget(self, static_tokens: int) -> int:
        """Calculate how many tokens are available for history.

        Token budget breakdown:
        - context_window = input + reasoning + output
        - Reserve max_output_tokens for output
        - Reserve a fraction for reasoning (CoT)
        - Reserve space for truncation notices (worst case)
        - The rest is available for input (static prompt + history)

        Args:
            static_tokens: Tokens used by the static prompt.

        Returns:
            Number of tokens available for history content.
        """
        model_spec = self._llm.model_spec

        # Total available for input = context_window - output - reasoning_buffer
        reasoning_buffer = int(model_spec.context_window * self.REASONING_BUFFER_FRACTION)
        max_input_tokens = (
            model_spec.context_window - model_spec.max_output_tokens - reasoning_buffer
        )

        # Reserve space for truncation notices
        truncation_reserve = SpotlightHistory.get_truncation_reserve_tokens(self._tokenizer)

        # Budget for history = input budget - static prompt - truncation reserve
        budget = max_input_tokens - static_tokens - truncation_reserve

        # Ensure non-negative
        return max(0, budget)
