"""Spotlight search planner.

The planner builds search plans by analyzing the user query, collection metadata,
and search history. It uses an LLM with structured output to generate plans.
"""

from pathlib import Path

from airweave.core.logging import ContextualLogger
from airweave.search.spotlight.external.llm.interface import SpotlightLLMInterface
from airweave.search.spotlight.external.tokenizer.interface import SpotlightTokenizerInterface
from airweave.search.spotlight.schemas.plan import SpotlightPlan
from airweave.search.spotlight.schemas.state import SpotlightState


class SpotlightPlanner:
    """Plans search queries based on state and history.

    The planner:
    1. Builds a prompt from static context, collection metadata, user query, and history
    2. Manages token budget to fit history within context window
    3. Calls the LLM to generate a structured search plan
    """

    # Path to context markdown files (relative to this module)
    CONTEXT_DIR = Path(__file__).parent.parent / "context"

    # Reserve this fraction of context_window for reasoning (CoT)
    REASONING_BUFFER_FRACTION = 0.15

    # Messages used when history is truncated (pre-defined so we can count their tokens)
    TRUNCATION_NOTICE = "\n*(Earlier iterations truncated to fit context window)*"
    HISTORY_FULLY_TRUNCATED_NOTICE = (
        "*(Search history exists but was truncated to fit context window. "
        "Previous iterations were performed but details are not shown.)*"
    )

    def __init__(
        self,
        llm: SpotlightLLMInterface,
        tokenizer: SpotlightTokenizerInterface,
        logger: ContextualLogger,
    ) -> None:
        """Initialize the planner.

        Args:
            llm: LLM interface for generating plans.
            tokenizer: Tokenizer for counting tokens.
            logger: Logger for debugging.
        """
        self._llm = llm
        self._tokenizer = tokenizer
        self._logger = logger

        # Load static context files once
        self._airweave_overview = self._load_context_file("airweave_overview.md")
        self._planner_task = self._load_context_file("planner_task.md")

        # Pre-calculate tokens for truncation messages (used in budget calculation)
        self._truncation_notice_tokens = self._tokenizer.count_tokens(self.TRUNCATION_NOTICE)
        self._fully_truncated_notice_tokens = self._tokenizer.count_tokens(
            self.HISTORY_FULLY_TRUNCATED_NOTICE
        )

    def _load_context_file(self, filename: str) -> str:
        """Load a context markdown file.

        Args:
            filename: Name of the file in the context directory.

        Returns:
            Contents of the file.

        Raises:
            FileNotFoundError: If the file doesn't exist.
        """
        filepath = self.CONTEXT_DIR / filename
        return filepath.read_text()

    async def plan(self, state: SpotlightState) -> SpotlightPlan:
        """Generate a search plan based on the current state.

        Args:
            state: The current spotlight state.

        Returns:
            A search plan generated by the LLM.
        """
        prompt = self._build_prompt(state)

        # Log only the dynamic parts (not static airweave_overview and planner_task)
        history_section = self._build_history_section(state, self._calculate_history_budget(0))
        self._log_dynamic_context(state, history_section)

        prompt_tokens = self._tokenizer.count_tokens(prompt)
        self._logger.debug(f"[Planner] Total prompt: {prompt_tokens:,} tokens")

        plan = await self._llm.structured_output(prompt, SpotlightPlan)

        # Log the generated plan
        self._logger.info(f"[Planner] Generated plan:\n{plan.to_md()}")

        return plan

    def _log_dynamic_context(self, state: SpotlightState, history_section: str) -> None:
        """Log the dynamic parts of the prompt context.

        Only logs user query, collection metadata, iteration, and history.
        Static parts (airweave_overview, planner_task) are not logged.
        """
        self._logger.debug(
            f"[Planner] Dynamic context for iteration {state.iteration_number}:\n"
            f"{'=' * 60}\n"
            f"USER QUERY: {state.user_query}\n"
            f"{'=' * 60}\n"
            f"COLLECTION METADATA:\n{state.collection_metadata.to_md()}\n"
            f"{'=' * 60}\n"
            f"HISTORY:\n{history_section}\n"
            f"{'=' * 60}"
        )

    def _build_prompt(self, state: SpotlightState) -> str:
        """Build the full prompt for the LLM.

        Token budget is managed to fit history within the context window.
        History is added from most recent to oldest until budget is exhausted.

        Args:
            state: The current spotlight state.

        Returns:
            The complete prompt string.
        """
        # Build the static part of the prompt (without history)
        static_prompt = self._build_static_prompt(state)
        static_tokens = self._tokenizer.count_tokens(static_prompt)

        # Calculate token budget for history
        budget_for_history = self._calculate_history_budget(static_tokens)

        # Build history section with budget
        history_section = self._build_history_section(state, budget_for_history)

        # Combine into final prompt
        return f"""{static_prompt}

---

## Search History

{history_section}
"""

    def _build_static_prompt(self, state: SpotlightState) -> str:
        """Build the static part of the prompt (everything except history).

        Args:
            state: The current spotlight state.

        Returns:
            The static prompt string.
        """
        return f"""# Airweave Overview

{self._airweave_overview}

---

{self._planner_task}

---

## Context for This Search

### User Query

{state.user_query}

### Collection Information

{state.collection_metadata.to_md()}

### Current Iteration

This is iteration **{state.iteration_number}**."""

    def _calculate_history_budget(self, static_tokens: int) -> int:
        """Calculate how many tokens are available for history.

        Token budget breakdown:
        - context_window = input + reasoning + output
        - Reserve max_output_tokens for output
        - Reserve a fraction for reasoning (CoT)
        - Reserve space for truncation notices (worst case)
        - The rest is available for input (static prompt + history)

        Args:
            static_tokens: Tokens used by the static prompt.

        Returns:
            Number of tokens available for history content.
        """
        model_spec = self._llm.model_spec

        # Total available for input = context_window - output - reasoning_buffer
        reasoning_buffer = int(model_spec.context_window * self.REASONING_BUFFER_FRACTION)
        max_input_tokens = (
            model_spec.context_window - model_spec.max_output_tokens - reasoning_buffer
        )

        # Reserve space for truncation notices (use the larger of the two)
        truncation_reserve = max(
            self._truncation_notice_tokens,
            self._fully_truncated_notice_tokens,
        )

        # Budget for history = input budget - static prompt - truncation reserve
        budget = max_input_tokens - static_tokens - truncation_reserve

        # Ensure non-negative
        return max(0, budget)

    def _build_history_section(self, state: SpotlightState, budget: int) -> str:
        """Build the history section within the token budget.

        History is added from most recent to oldest until budget is exhausted.
        The budget already accounts for truncation notice tokens, so we're
        guaranteed to fit within the context window.

        Args:
            state: The current spotlight state.
            budget: Maximum tokens for history content (excludes truncation notices).

        Returns:
            Markdown string with history (or message if first iteration).
        """
        # First iteration - no history yet
        if state.history is None:
            return "No search history yet. This is the first iteration."

        # Build history from most recent to oldest
        history_parts: list[str] = []
        tokens_used = 0

        for iter_num, iteration in state.history.iter_recent_first():
            iter_md = iteration.to_md(iter_num)
            iter_tokens = self._tokenizer.count_tokens(iter_md)

            # Check if adding this iteration would exceed budget
            if tokens_used + iter_tokens > budget:
                # Add truncation notice (tokens already reserved in budget calculation)
                if history_parts:
                    history_parts.append(self.TRUNCATION_NOTICE)
                break

            history_parts.append(iter_md)
            tokens_used += iter_tokens

        # If we couldn't fit any history, indicate that (tokens already reserved)
        if not history_parts:
            return self.HISTORY_FULLY_TRUNCATED_NOTICE

        # Join with separators (most recent first)
        return "\n\n---\n\n".join(history_parts)
